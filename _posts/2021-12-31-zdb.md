---
layout: post
title: "Integrating library data into authority file"
subtitle: "The challenges of MARC XML and inconsistent transcription practices"
author: Till Grallert
date: 2021-12-31
ORCID: orcid.org/0000-0002-5739-8094
categories: blog
thumbnail-img: "https://zdb-katalog.de/javax.faces.resource/images/zdb_logo_weiss.svg.xhtml?ln=frontend&ver=1.16.0"
tags:
   authority files
   linked open data
---

A recent Twitter post from my former colleague Anne Klammt made me aware of a recent relaunch of "[Zeitschriftendatenbank](https://zeitschriftendatenbank.de)" (ZDB), the portal for periodical holdings in German (and Austrian) libraries. 

<blockquote class="twitter-tweet" data-partner="tweetdeck"><p lang="en" dir="ltr">Just played around with the German Union Catalogue of Serails (ZDB) <a href="https://twitter.com/DNB_Aktuelles?ref_src=twsrc%5Etfw">@DNB_Aktuelles</a> Nice graphs, maps and data. <a href="https://twitter.com/hashtag/journals?src=hash&amp;ref_src=twsrc%5Etfw">#journals</a> <a href="https://twitter.com/hashtag/linkeddata?src=hash&amp;ref_src=twsrc%5Etfw">#linkeddata</a> <a href="https://twitter.com/hashtag/authorityfiles?src=hash&amp;ref_src=twsrc%5Etfw">#authorityfiles</a> <a href="https://t.co/f0qVtAWA5d">pic.twitter.com/f0qVtAWA5d</a></p>&mdash; Dr. Anne Klammt (@archaeoklammt) <a href="https://twitter.com/archaeoklammt/status/1465757321464889352?ref_src=twsrc%5Etfw">November 30, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


Part of the German National Library (GND), the website looks great and provides a lot of data-driven functionality, such as maps and timelines of holdings. The display language of the website itself, though not the bibliographic data, can be toggled between German and English. While this is a welcome nod to international users, I have to immediately note that both the data set and the interface exclude communities, particularly from the Global South, whose languages are not written in Latin script but whose cultural heritage is nevertheless held by institutions in the Global North. This is especially relevant for Arabic material, as I will outline in the section on transliterations below.

![The ZDB interface showing details for the journal *al-Jinān*](/assets/images/zdb_interface.png)

On the upside, however, ZDB supports historical political entities, such as the Ottoman Empire, for facetted browsing, which I have not yet seen in other library catalogues. 

Crucially, ZDB also provides open access to machine-actionable data through the website and, crucially, [a number of application programming interfaces](https://zeitschriftendatenbank.de/services/schnittstellen/sru) (APIs). I had recently worked on mapping MARC XML to TEI (there should be another post on this) and was curious how much holding data for Arabic periodicals I could gather for our [Project Jarāʾid](https://projectjaraid.github.io) data set through the ZDB APIs. As it turned out, a lot, which allowed me to integrate fairly comprehensive information on German holding institutions! Note that I have finally moved away from the rudimentary data model upon which the Jarāʾid website is based. The new data model is built around `<tei:biblStruct>` and the data set has consequently moved to the [repository of authority files for OpenArabicPE](https://github.com/openarabicpe/authority-files). Eventually, this data will find its way to [Project Jarāʾid](https://projectjaraid.github.io).

# Querying ZDB through SRU

Querying ZDB through SRU is rather simple and has the advantage of being accessible  without any scripting through the web browser. I am looking for holding data of periodicals published before 1930---information that is unlikely to change often or frequently---and I do not foresee an need to regularly update data pulled from ZDB. In case this will be necessary in the future, we can simply diff the old and new results and only integrate the resulting changes.

The SRU interface is [well documented](https://zeitschriftendatenbank.de/services/schnittstellen/sru) and one can easily query ZDB for Arabic periodicals published between 1780 and 1929 using the following query: `https://services.dnb.de/sru/zdb?version=1.1&operation=searchRetrieve&query=spr%3Dara%20and%20eje%3e1780%20and%20eje%3c1929%20sortby%20eje/sort.ascending&recordSchema=MARC21-xml&maximumRecords=100`. There are a couple of observations to be made:

1. The number of returned records is limited to 100 and one has to make use of `startRecord=\d` to access records beyond 100.
2. Limiting the year of first publication (`eje`) to a range between minimum (`%3c`) and maximum (`%3e`) removes all periodicals with unknown publications dates, which have been catalogued as published in 0000. The common cataloguing practice of assigning the year 1900 to such cases was not observed in the ZDB data.

# Available data and data quality

The SRU query returns MARC XML with all the relevant bibliographic information for reconciliation. However, a major piece of information, the one I was actually looking for, was missing. Holding information is absent from the data returned via SRU. Some brief Twitter communication with [Reinhold Heuvelmann](https://twitter.com/sollbruchstelle) later, it turned out that GND had devised a new MARC field (924) for holding information and that this field was available only through the individual MARC XML files for ZDB IDs provided at the URL `http://ld.zdb-services.de/data/{ZDB ID}.plus-1.mrcx`. As the ZDB ID is part of the data available through SRU, I could script the query of holding data for each item, which, in turn, required another query to a linked-open-data resource for dereferencing the ISIL codes through `http://ld.zdb-services.de/data/organisations/{ISIL}.rdf` for holding institutions and for getting their geographic locations (used for the maps on the ZDB website).

The final XSLT for transforming the MARC XML returned via SRU into TEI `<biblStruct>` nodes including holding information can be found in [this GitHub repository](https://github.com/openarabicpe/convert_tei-to-bibliographic-data).

Looking at the cataloguing data a number of issues became immediately apparent: 

1. The ZDB does not provide information in the original script but uses transliterations into Latin script. We need to reconstruct the original script in order to reconcile the ZDB data with our data set.
2. The quality and consistency of transliterations(!) varies considerably and complicates reconciliation with other authority files.
3. Use of unicode is a mixed bag: some entries use composed, others decomposed unicode. This can be normalized with the `normalize-unicode(., 'NFKC')` function.
3. Languages and scripts are used synonymously. Thus, material in Arabic script, such as Ottoman Turkish, Pashto, Farsi, Uzbek etc., will be included in the data set.
4. Dates in MARC field 363 are recorded without the corresponding calendar. Thus, one finds a mix of at least Islamic, Jewish, and Gregorian dates.

On the upside, ZDB provides OCLC identifiers 

## Transliteration of languages written in non-Latin scripts

There is a long standing tradition to transliterate non-Latin scripts to Latin based on the language and using sets of rules/best practices involving an extended Latin alphabet with diacritics. In theory, the rules should be obvious and correctly and consistently applied. There should also be a technological stack that allows for the correct application of the rules. 

1. *obvious*: i.e. the ones published by the relevant academic associations in the relevant field. In the case of Arabic, the transcription of the *Deutsche Morgenländische Gesellschaft* should apply.
2. *correctly* and *consistently applied*: don't change rules within records. Try to not change rules between records. The later is almost unavoidable as catalogues are historical artefacts with long compilation histories. Rules will inevitably change over time and old catalogue records will reflect these changes.

As it turned out none of this was the case for the ZDB data. It seemed as if every cataloguer had to cobble together their own combination of diacritics and that most cataloguers made frequent errors in applying "their" transliterations. Thus we find `Ḥarb al-ā̆lam` (should be *ḥarb al-ʿālam*), `ṣahāʾif`(*ṣaḥāʾif*), `rasmıya` (*rasmīya*), `ǧerîde jaumêje` (*ǧarīda yaumīya*), `bat̄t̄` (*baṯṯ*), `At-Ṯaqrir` (*at-taqrīr*), `w̌a-šahīrāt ṡaiyidāt ȧl-ʿālām` (*wa-šahīrāt saiyidāt al-ʿālam*), `muṣǎuwara` (*muṣauwara*) etc. in addition to common errors such as `Ḥadiqāt` (*ḥadīqat*) or `Maǧmūʾat` (*maǧmūʿat*).

This state of affairs necessitated some adaptations of our [code for re-transliterating the IJMES transliteration of Arabic into Arabic script](https://github.com/OpenArabicPE/authority-files/blob/master/xslt/tei-biblstruct_add-arabic-script.xsl) as well as a lot of data cleaning. The resulting TEI, however, is worth it and can easily be integrated into our authority files:

```xml
<biblStruct subtype="journal" type="periodical">
    <monogr>
       <title level="j" xml:lang="ar-Latn-x-dmg">Rauḍat al-madāris al-miṣrı̄ya</title>
       <!-- Arabic title linked to our authority file -->
       <title level="j" ref="oape:bibl:606" xml:lang="ar">روضة المدارس المصرية</title>
       <idno type="zdb">2062697-6</idno>
       <idno type="OCLC">645736081</idno>
       <idno type="zdb">2062697-6</idno>
       <textLang mainLang="ar"/>
       <imprint>
          <pubPlace> <placeName ref="geon:360630 jaraid:place:1 oape:place:226" xml:lang="ar-Latn-x-ijmes">al-Qāhira</placeName> <placeName ref="geon:360630 jaraid:place:1 oape:place:226" resp="#xslt" xml:lang="ar">القاهرة</placeName> </pubPlace>
          <publisher><orgName xml:lang="ar-Latn-x-dmg">Matbaʿat al-Madāris al Malakı̄ya</orgName></publisher>
          <date when="1871">1871</date>
          <!-- calendars have been established based on numerical ranges. dates were then normalised -->
          <date calendar="#cal_islamic" datingMethod="#cal_islamic" notAfter="1871-03-22" notBefore="1870-04-03" type="onset" when-custom="1287">1287</date>
          <date calendar="#cal_islamic" datingMethod="#cal_islamic" notAfter="1878-01-04" notBefore="1877-01-16" type="terminus" when-custom="1294">1294</date>
       </imprint>
    </monogr>
    <note type="comments"> 
    	<list xml:lang="de">
            <item>Repr.: al Qāhira : Dar al-Kutub wa-'l-Waṯāʾiq al-Qaumı̄ya, 1997/98</item>  
		</list> 
	</note>
    <!-- holding information -->
</biblStruct>
```

# Resulting coverage

Integrating the ZDB data has significantly expanded the holding information in our data set, which is demonstrated by the following map.

![Map of 2039 periodical holdings in 82 locations](/assets/maps/map-data-set-periodical-holdings-med-na_mapped.svg)


# Future work

The map shows the significance of the AUB library holdings in Beirut. We have to keep in mind, though, that holding data is difficult to get by. The ZDB data includes Worldcat IDs (`<idno type="OCLC">`), which should allow for querying Worldcat APIs. Unfortunately, Worldcat is famously protecting its data from all those unable (or unwilling) to pay and just [removed its "experimental" LOD service](https://web.archive.org/web/20220104155900/https://help.oclc.org/Discovery_and_Reference/WorldCat-org/Troubleshooting/What_happened_to_the_linked_data_that_previously_displayed_on_records_in_WorldCat.org?sl=en). Beyond the Arabic Union Catalogue, which is not sharing data through APIs, I am not aware of any other transregional or international catalogue. This means that we will have to contact and query national catalogues where they are available and library or project catalogues for all other regions. One of the latter is HathiTrust, which provides a data dump for non-members with information on individual digitised copies as CSV. 